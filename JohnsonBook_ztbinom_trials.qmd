---
title: "JohnsonBook_ztbinom_trials"
format: html
---

```{r}
library(tidyverse)
```

```{r}
d_ztbinom_book <- function(k, p, N) {
  q = p + 1
  
  prob = (1/(1-(q^-N))) * choose(n = N+k-1, k = N-1) * (p/q)^k * (1 - (p/q))^N
  
  return(prob)
}
```

```{r}
cdf_ztbinom_book <- function(k, p, N) {
  q = p + 1
  
  cumulative_prob <- 0
  for (i in 1:k) {
    cumulative_prob = cumulative_prob + d_ztbinom_book(i, p, N)
  }
  
  return(cumulative_prob)
}
```

```{r}
mean_ztbinom_book <- function(p, N) {
  q = p + 1
  
  mean = N*p / (1-(q^-N))
  
  return(mean)
}
```

```{r}
var_ztbinom_book <- function(p, N) {
  q = p + 1
  
  var = ( (N*p*q) / (1-(q^-N)) ) * ( 1 - (N*p / q) * ( 1 / (1-(q^-N)) - 1)  )
  
  return(var)
}
```

```{r}
d_ztbinom_book(k = 3, p = 0.5, N = 10)

cdf_ztbinom_book(k = 25, p = 0.5, N = 10)

mean_ztbinom_book(p = 0.5, N = 10)

var_ztbinom_book(p = 0.5, N = 10)
```

```{r}
# pdf build

pdf <- d_ztbinom_book(k = seq(1, 20, 1), p = 0.5, N = 10)

pdf_df <- as.data.frame(pdf)

pdf_df <- pdf_df |>
  mutate(k = seq(1, 20, 1)) |>
  rename(prob = pdf)

pdf_df |>
  ggplot(aes(x = k, y = prob)) +
  geom_line() +
  theme_bw() 

sum(pdf)
```
```{r}
cdf <- rep(0, 20)

for (i in 1:20) {
  cdf[i] <- cdf_ztbinom_book(k = i, p = 0.5, N = 10)
}

cdf_df <- as.data.frame(cdf)

cdf_df <- cdf_df |>
  mutate(k = seq(1, 20, 1)) |>
  rename(prob = cdf)
```

```{r}
cdf_df |>
  ggplot(aes(x = k, y = prob)) +
  geom_line() +
  theme_bw() +
  scale_x_continuous(breaks = seq(0, 20, 1)) +
  scale_y_continuous(breaks = seq(-0.1, 1, 0.1))
```

```{r}
r_ztbinom_book <- function(n, p, N) {
  
  draws <- rep(NA, n)
  
  upper = N*10
  
  min = cdf_ztbinom_book(k = 1, p = p, N = N)
  max = cdf_ztbinom_book(k = upper, p = p, N = N)
  
  cdf_vals <- rep(0, upper)
  
  for (i in 1:upper) {
    cdf_vals[i] <- cdf_ztbinom_book(k = i, p = p, N = N)
  }
  
  for (i in 1:n) {
    unif_draw <- runif(n = 1, min = min, max = max)
    
    dist_to_0 = cdf_vals - unif_draw
    
    draw <- which.min(abs(dist_to_0))
    
    draws[i] <- draw
  }
  return(draws)
}
```

```{r}
set.seed(1)

n = 100000
N = 18
p = 0.75

# pdf <- d_ztbinom_book(k = seq(1, N*10, 1), p = p, N = N)
# 
# pdf_df <- as.data.frame(pdf)
# 
# pdf_df <- pdf_df |>
#   mutate(k = seq(1, N*10, 1)) |>
#   rename(prob = pdf)
# 
# pdf_df |>
#   ggplot(aes(x = k, y = prob)) +
#   geom_line() +
#   theme_bw() 

d <- r_ztbinom_book(n, p, N)

draws <- as.data.frame(d) |>
  rename(draws = d) 

draws |>
  ggplot(aes(x = draws)) +
  geom_histogram(binwidth = 1) +
  theme_bw() 

mean(draws$draws)
var(draws$draws)
```

```{r}
pdf <- d_ztbinom_book(k = seq(1, 20, 1), p = 0.5114, N = 9.2648)

pdf_df <- as.data.frame(pdf)

pdf_df <- pdf_df |>
  mutate(k = seq(1, 20, 1)) |>
  rename(prob = pdf)

pdf_df |>
  ggplot(aes(x = k, y = prob)) +
  geom_line() +
  theme_bw() 

# sum(pdf)
```

$$
0 = NP(1+P)^{4N+1} + N^2P^2(1+P)^{4N} - 2NP(1+P)^{3N+1} - (1+P)^{3N+1} - 2N^2P^2(1+P)^{3N}
 - NP(1+P)^{3N} - 2N^2P^2(1+P)^{3N-1} + NP(1+P)^{2N+1} - 2(1+P)^{2N+1}
 + 2N^2P^2(1+P)^{2N} + 5NP(1+P)^{2N} + 4N^2P^2(1+P)^{2N-1} - 2NP(1+P)^{N+1}
 + (1+P)^{N+1} - 2N^2P^2(1+P)^{N} - NP(1+P)^{N} - 2N^2P^2(1+P)^{N-1}
 + NP(1+P) + N^2P^2
$$

$$
\frac{NP(1+P)}{1 - (1+P)^{-N}}
\left[
1 - \left(\frac{NP}{1+P}\right)
\left(\frac{1}{1 - (1+P)^{-N}} - 1\right)
\right]
=
\left(NP(1+P) + (NP)^2\right)
\frac{1}{1 - (1+P)^{-N}}
- \left(
\frac{NP}{1 - (1+P)^{-N}}
\right)^2

$$

```{r}
polynomial <- function(N, P) {
  0 ==
    N*P*(1+P)^(4*N+1) +
    N^2*P^2*(1+P)^(4*N) -
    2*N*P*(1+P)^(3*N+1) -
    (1+P)^(3*N+1) -
    2*N^2*P^2*(1+P)^(3*N) -
    N*P*(1+P)^(3*N) -
    2*N^2*P^2*(1+P)^(3*N-1) +
    N*P*(1+P)^(2*N+1) -
    2*(1+P)^(2*N+1) +
    2*N^2*P^2*(1+P)^(2*N) +
    5*N*P*(1+P)^(2*N) +
    4*N^2*P^2*(1+P)^(2*N-1) -
    2*N*P*(1+P)^(N+1) +
    (1+P)^(N+1) -
    2*N^2*P^2*(1+P)^(N) -
    N*P*(1+P)^(N) -
    2*N^2*P^2*(1+P)^(N-1) +
    N*P*(1+P) +
    N^2*P^2
}

# denom <- (1 - (1+P)^(-N))
# 
# mean_estimator <- function(N, P) {
#   calc_mean <- (N*P) / denom)
#   return(calc_mean)
# }
# 
# 
# variance_estimator <- function(N, P) {
#   calc_var <- 
#     (N*P*(1+P) / denom) * 
#     (
#       1 - ((N*P)/1+P) * ((1 / (denom)) - 1)
#     )
#   return(calc_var)
# }
```

```{r}
# mean_estimator(10, 0.5)
# 
# variance_estimator(10, 0.5)
```


# Optimization
```{r}
# ---------------------------------------------------------
# 1. The "Forward" Theoretical Functions (Eq 41.1 & 41.2)
# ---------------------------------------------------------

mean_tnb <- function(N, P) {
  Q <- 1 + P
  denom <- 1 - Q^(-N)
  if (abs(denom) < 1e-15) return(Inf)
  
  # Eq 41.1
  return( (N * P) / denom )
}

variance_tnb <- function(N, P) {
  Q <- 1 + P
  denom <- 1 - Q^(-N)
  if (abs(denom) < 1e-15) return(Inf)
  
  # Eq 41.2 (Consolidated)
  # Var = [Mean_Untruncated * Q / Denom] * [ 1 - (Mean_Untruncated/Q) * (1/Denom - 1) ]
  
  term_lead <- (N * P * Q) / denom
  term_bracket <- 1 - ( (N * P) / Q ) * ( (1/denom) - 1 )
  
  return( term_lead * term_bracket )
}
```

```{r}
# ---------------------------------------------------------
# 2. The Optimizer (Solving for N, P)
# ---------------------------------------------------------

find_parameters <- function(observed_mean, observed_var) {
  
  # Objective: Minimize the difference between Theoretical (Eq 41) and Observed
  objective_fn <- function(params) {
    N <- params[1]
    P <- params[2]
    
    theo_mean <- mean_tnb(N, P)
    theo_var  <- variance_tnb(N, P)
    
    # Squared Error Cost
    error <- (theo_mean - observed_mean)^2 + (theo_var - observed_var)^2
    return(error)
  }
  
  # Initial Guess
  start_guesses <- c(10, 0.5)
  
  result <- optim(
    par = start_guesses,
    fn = objective_fn,
    method = "L-BFGS-B",
    lower = c(0.001, 0.001), # Parameters must be positive
    upper = c(Inf, Inf)
  )
  
  return(list(
    Estimated_N = result$par[1],
    Estimated_P = result$par[2],
    Goodness_of_Fit = result$value # Closer to 0 is better
  ))
}
```

```{r}
find_parameters(3, 4)
```


```{r}
d <- r_ztbinom_book(n = 10000, p = 0.8, N = 3.17)

draws <- as.data.frame(d) |>
  rename(draws = d) 

draws |>
  ggplot(aes(x = draws)) +
  geom_histogram(binwidth = 1) +
  theme_bw() 

mean(draws$draws)
var(draws$draws)
```

# Option to replace my current rztbinom function

The short answer is that in R, the choose(n, k) function already handles fractional values automatically! It uses the Generalized Gamma function definition of factorials under the hood.However, relying on choose() for distributions is numerically unstable (it can result in huge numbers that overflow).Here is the breakdown of how to handle fractional $N$ robustly, and how to drastically speed up your sampler by realizing this distribution exists in base R.1. The Robust Way to Calculate "Choose" (Log-Gamma)Mathematically, for fractional $n$, the factorial $n!$ is replaced by $\Gamma(n+1)$.$$\binom{n}{k} = \frac{\Gamma(n+1)}{\Gamma(k+1)\Gamma(n-k+1)}$$In R, you should use lgamma() (the log of the Gamma function) to avoid massive numbers during intermediate calculation steps:R# Instead of choose(N + k - 1, N - 1)
# Use this:
val <- lgamma(N + k) - lgamma(N) - lgamma(k + 1)
coeff <- exp(val)
2. The "R" Way (Drastically Faster)Your distribution is the Zero-Truncated Negative Binomial.R already has highly optimized C-code for the Negative Binomial (dnbinom, qnbinom, etc.) which natively accepts fractional N (the size parameter).You just need to map your textbook parameters ($N, P$) to R's parameters ($size, prob$).Textbook $N$ $\rightarrow$ R sizeTextbook $P$ $\rightarrow$ R prob $= \frac{1}{1+P}$The Optimized SamplerInstead of building a CDF loop (which is slow), we can use Inverse Transform Sampling on the untruncated Negative Binomial, but forced to skip zero.The Logic:Calculate the probability of Zero in the untruncated world ($p_0$).Draw a uniform random number $u$ between $p_0$ and 1.Find the quantile of $u$ using the standard Negative Binomial function.Here is the complete, optimized code replacing your three functions:R# 1. The Probability Mass Function (PMF)
# Replaces d_ztbinom_book
d_ztbinom_optim <- function(x, p, N) {
  # Convert Textbook P to R's prob
  r_prob <- 1 / (1 + p)
  
  # Calculate untruncated probability
  # R handles fractional N (size) automatically here
  num <- dnbinom(x, size = N, prob = r_prob)
  
  # Calculate probability of 0
  prob_zero <- dnbinom(0, size = N, prob = r_prob)
  
  # Adjust for zero-truncation
  return(num / (1 - prob_zero))
}

# 2. The Sampler
# Replaces r_ztbinom_book
r_ztbinom_optim <- function(n, p, N) {
  # Convert Textbook P to R's prob
  r_prob <- 1 / (1 + p)
  
  # 1. Calculate the "floor" probability (probability of seeing 0)
  # This defines the lower bound of our cumulative probability
  p0 <- dnbinom(0, size = N, prob = r_prob)
  
  # 2. Generate Uniform random numbers between p0 and 1
  # This effectively skips the "0" part of the CDF
  u <- runif(n, min = p0, max = 1)
  
  # 3. Use R's quantile function to map these probabilities back to integers
  # This is vectorised and infinitely faster than a loop
  draws <- qnbinom(u, size = N, prob = r_prob)
  
  return(draws)
}
Why this works for Fractional NIf you check help(dnbinom) in R, it states:"size: target for number of successful trials, or dispersion parameter (the shape parameter of the gamma mixing distribution)."Since the Negative Binomial is a Gamma-Poisson mixture, the size ($N$) parameter is strictly a shape parameter in the Gamma function. It does not need to be an integer.Testing with Fractional NYou can run this snippet to verify it handles fractions correctly:R# Test with Fractional N = 5.5
set.seed(123)
N_frac <- 5.5
P_val <- 0.5

# Generate 10,000 samples
samples <- r_ztbinom_optim(n = 10000, p = P_val, N = N_frac)

# Check the Mean
obs_mean <- mean(samples)

# Theoretical Mean (Eq 41.1 from your book)
# Mean = NP / (1 - (1+P)^-N)
denom <- 1 - (1 + P_val)^(-N_frac)
theo_mean <- (N_frac * P_val) / denom

cat("Observed Mean:", obs_mean, "\n")
cat("Theoretical Mean:", theo_mean, "\n")













