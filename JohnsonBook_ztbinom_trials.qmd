---
title: "JohnsonBook_ztbinom_trials"
format: html
editor: 
  markdown: 
    wrap: 72
---

$$
0 = NP(1+P)^{4N+1} + N^2P^2(1+P)^{4N} - 2NP(1+P)^{3N+1} - (1+P)^{3N+1} - 2N^2P^2(1+P)^{3N}
 - NP(1+P)^{3N} - 2N^2P^2(1+P)^{3N-1} + NP(1+P)^{2N+1} - 2(1+P)^{2N+1}
 + 2N^2P^2(1+P)^{2N} + 5NP(1+P)^{2N} + 4N^2P^2(1+P)^{2N-1} - 2NP(1+P)^{N+1}
 + (1+P)^{N+1} - 2N^2P^2(1+P)^{N} - NP(1+P)^{N} - 2N^2P^2(1+P)^{N-1}
 + NP(1+P) + N^2P^2
$$

\$\$

\frac{NP(1+P)}{1 - (1+P)^{-N}} \left[
1 - \left(\frac{NP}{1+P}\right)
\left(\frac{1}{1 - (1+P)^{-N}} - 1\right)
\right]= \left(NP(1+P) + (NP)\^2\right) \frac{1}{1 - (1+P)^{-N}} -
\left( \frac{NP}{1 - (1+P)^{-N}} \right)\^2

\$\$

```{r}
library(tidyverse)
library(knitr)
```

```{r}
# mychoose <- function(r, k){
#   ifelse(k <= 0, (k == 0),
#          sapply(k, function(k) prod(r:(r-k+1))) / factorial(k))
# }
```

```{r}
# # 1. Define the logic for a single calculation (Scalar version)
# mychoose_scalar <- function(r, k) {
#   if (k < 0) {
#     return(0)
#   } else if (k == 0) {
#     return(1)
#   } else {
#     # Calculate the falling factorial: r * (r-1) * ... * (r-k+1)
#     # We use seq() instead of ':' to be safe with fractional steps if needed
#     numerator <- prod(seq(from = r, by = -1, length.out = k))
#     return(numerator / factorial(k))
#   }
# }
# 
# # 2. Convert it to a function that accepts vectors
# mychoose <- Vectorize(mychoose_scalar)
# 
# # --- Test ---
# # Example from your screenshot context
# N <- 10
# k_vec <- seq(1, 20, 1)
# 
# # Now this will work without the "only first element used" warning
# r_vec <- N + k_vec - 1
# k_const <- N - 1
# 
# result <- mychoose(r = r_vec, k = k_const)
# print(head(result))
```

choose function based on
https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/Special
where they talk about the gamma functions being approximations of the
binomial coefficient. I used the log version for numerical stability

```{r}
generalized_choose <- function(n, k) {
  return(exp(lgamma(n + 1) - lgamma(k + 1) - lgamma(n - k + 1)))
}
```

Test cases for the new function

```{r}
choose(10, 3)
generalized_choose(10, 3)
```

```{r}
choose(12, 4)
generalized_choose(12.000001, 4.0000001)
```

use that generalized function for the functions

```{r}
d_ztbinom_book <- function(k, p, N) {
  q = p + 1
  
  prob = (1/(1-(q^-N))) * generalized_choose(n = N+k-1, k = N-1) * (p/q)^k * (1 - (p/q))^N
  
  return(prob)
}
```

```{r}
cdf_ztbinom_book <- function(k, p, N) {
  q = p + 1
  
  cumulative_prob <- 0
  for (i in 1:k) {
    cumulative_prob = cumulative_prob + d_ztbinom_book(i, p, N)
  }
  
  return(cumulative_prob)
}
```

```{r}
mean_ztbinom_book <- function(p, N) {
  q = p + 1
  
  mean = N*p / (1-(q^-N))
  
  return(mean)
}
```

```{r}
var_ztbinom_book <- function(p, N) {
  q = p + 1
  
  var = ( (N*p*q) / (1-(q^-N)) ) * ( 1 - (N*p / q) * ( 1 / (1-(q^-N)) - 1)  )
  
  return(var)
}
```

![](images/clipboard-2252960977.png){width="481"}

tests the functions with a specific use case

```{r}
d_ztbinom_book(k = 3, p = 0.5, N = 10)

cdf_ztbinom_book(k = 25, p = 0.5, N = 10)

mean_ztbinom_book(p = 0.5, N = 10)

var_ztbinom_book(p = 0.5, N = 10)
```

builds a full pdf to confirm the shape and behavrio of the calculated
distribution

```{r}
# pdf build

pdf <- d_ztbinom_book(k = seq(1, 20, 1), p = 0.5, N = 10)

pdf_df <- as.data.frame(pdf)

pdf_df <- pdf_df |>
  mutate(k = seq(1, 20, 1)) |>
  rename(prob = pdf)

pdf_df |>
  ggplot(aes(x = k, y = prob)) +
  geom_line() +
  theme_bw() 

sum(pdf)
```

Same for the CDF

```{r}
cdf <- rep(0, 20)

for (i in 1:20) {
  cdf[i] <- cdf_ztbinom_book(k = i, p = 0.5, N = 10)
}

cdf_df <- as.data.frame(cdf)

cdf_df <- cdf_df |>
  mutate(k = seq(1, 20, 1)) |>
  rename(prob = cdf)

cdf_df |>
  ggplot(aes(x = k, y = prob)) +
  geom_line() +
  theme_bw() +
  scale_x_continuous(breaks = seq(0, 20, 1)) +
  scale_y_continuous(breaks = seq(-0.1, 1, 0.1))
```

uses those functions to create a value-generating function for the
ztnbinom from n and p

Things to fix:

use which.max to find the next highest value after sampling from 0 to the cdf value at N*10

```{r}
r_ztbinom_book <- function(n, p, N) {
  
  set.seed(7)
  
  # 'price is right' function
  # takes a value x, and a vector y, finds and returns the y value for which
  # x is closest to without going over
  
  pir <- function(x, y){
    
    val <- min(y[y >= x])

    # y <- y - x
    # val <- min(y[y > 0])
    # val <- val + x
    return(val)
  }

  # sets upper bound for how many cdf values to calculate, keeps sampler fast
  upper = N*10
  
  cdf_vals <- rep(0, upper)
  return_vals <- rep(0, upper)
  
  for (i in 1:upper) {
    return_vals[i] <- i
    cdf_vals[i] <- cdf_ztbinom_book(k = i, p = p, N = N)
  }
  
  tab <- tibble(return_vals, cdf_vals)
  
  
  # sets bounds for upper and lower limit of the unif() draw function for the 
  # inverse cdf sampler
  
  # min = cdf_ztbinom_book(k = 1, p = p, N = N)
  min = 0
  max = cdf_ztbinom_book(k = upper, p = p, N = N) # can do e^huge
  draws <- rep(0, n)
  
  for (i in 1:n) {
    unif_draw <- runif(n = 1, min = min, max = max)
    
    # which value of the cdf is just higher than this value
    
    cdf_val_of_draw <- pir(unif_draw, cdf_vals)
    
    draw <- tab$return_vals[tab$cdf_vals == cdf_val_of_draw]
    
    draws[i] <- draw
  }
  return(draws)
}
```


```{r}
set.seed(1)

n = 100000
N = 10
p = 0.5

# pdf <- d_ztbinom_book(k = seq(1, N*10, 1), p = p, N = N)
# 
# pdf_df <- as.data.frame(pdf)
# 
# pdf_df <- pdf_df |>
#   mutate(k = seq(1, N*10, 1)) |>
#   rename(prob = pdf)
# 
# pdf_df |>
#   ggplot(aes(x = k, y = prob)) +
#   geom_line() +
#   theme_bw() 

d <- r_ztbinom_book(n, p, N)

test_draws <- as.data.frame(d) |>
  rename(test_draws = d) 

# draws |>
#   ggplot(aes(x = draws)) +
#   geom_histogram(binwidth = 1) +
#   theme_bw() 

mean(test_draws$test_draws)
var(test_draws$test_draws)
```

```{r}
# # Fixed parameters
# n_samples <- 10000 # Number of draws per iteration
# N_fixed   <- 3   # Fixed N parameter
# 
# # Range for p (99 sets of calculations from 0.01 to 0.99)
# p_values <- seq(from = 0.01, to = 0.99, by = 0.01)
# 
# # Initialize an empty list to store results
# results_list <- list()
# 
# ## ---------------------------------------------------------
# ## 2. Iteration Loop (Using Original Function)
# ## ---------------------------------------------------------
# 
# for (i in seq_along(p_values)) {
#   current_p <- p_values[i]
# 
#   # 1. Generate the truncated random data using the ORIGINAL function
#   simulated_data <- r_ztbinom_book(n = n_samples, p = current_p, N = N_fixed)
# 
#   # 2. Calculate observed moments
#   obs_mean <- mean(simulated_data)
#   obs_var  <- var(simulated_data)
# 
#   # Avoid division by zero
#   if (obs_var > 1e-9) {
#     mean_to_var_ratio <- obs_mean / obs_var
#   } else {
#     mean_to_var_ratio <- NA
#   }
# 
#   # 3. Store the results
#   results_list[[i]] <- tibble(
#     p_param = current_p,
#     N_param = N_fixed,
#     Observed_Mean = obs_mean,
#     Observed_Variance = obs_var,
#     Mean_to_Var_Ratio = mean_to_var_ratio
#   )
# 
# }
# 
# # Combine results into a final data frame
# simulation_results <- bind_rows(results_list)
# 
# here::here(saveRDS(simulation_results, file = "simulation_results3.RDS"))
```

```{r}
simulation_results100 <- readRDS("~/Documents/St. Andrews/2023 Internship/Simulated_Data/simulation_results.RDS")

simulation_results20 <- readRDS("~/Documents/St. Andrews/2023 Internship/Simulated_Data/simulation_results20.RDS")

simulation_results3 <- readRDS("~/Documents/St. Andrews/2023 Internship/Simulated_Data/simulation_results3.RDS")
```

```{r}
reg_try <- simulation_results100 |>
  filter(Mean_to_Var_Ratio < 1)
## ---------------------------------------------------------
## 3. Create the Plot
## ---------------------------------------------------------

dispersion_plot <- reg_try |>
  ggplot(aes(x = p_param, y = Mean_to_Var_Ratio)) +
  geom_point(size = 1, alpha = 0.6, color = "darkred") +
  geom_line(color = "darkred", alpha = 0.7) +
  geom_smooth(method = "lm") +
  
  labs(
    title = paste0("Dispersion Ratio vs. Parameter p"),
    subtitle = "Ratio of Observed Mean to Observed Variance using non-vectorized simulation",
    x = "Parameter p (0.01 to 0.99)",
    y = "Mean / Variance Ratio (Dispersion)"
  ) +
  theme_bw()

# Print the plot
print(dispersion_plot)

# Display the first few results rows
head(simulation_results3)
```

```{r}
# reg_try <- simulation_results |>
#   filter(Mean_to_Var_Ratio < 1)

lm(reg_try$p_param ~ reg_try$Mean_to_Var_Ratio)
```

```{r}
# pdf <- d_ztbinom_book(k = seq(1, 20, 1), p = 0.5, N = 10)
# 
# pdf_df <- as.data.frame(pdf)
# 
# pdf_df <- pdf_df |>
#   mutate(k = seq(1, 20, 1)) |>
#   rename(prob = pdf)
# 
# pdf_df |>
#   ggplot(aes(x = k, y = prob)) +
#   geom_line() +
#   theme_bw() 
# 
# sum(pdf)
```

```{r}
# polynomial <- function(N, P) {
#   0 ==
#     N*P*(1+P)^(4*N+1) +
#     N^2*P^2*(1+P)^(4*N) -
#     2*N*P*(1+P)^(3*N+1) -
#     (1+P)^(3*N+1) -
#     2*N^2*P^2*(1+P)^(3*N) -
#     N*P*(1+P)^(3*N) -
#     2*N^2*P^2*(1+P)^(3*N-1) +
#     N*P*(1+P)^(2*N+1) -
#     2*(1+P)^(2*N+1) +
#     2*N^2*P^2*(1+P)^(2*N) +
#     5*N*P*(1+P)^(2*N) +
#     4*N^2*P^2*(1+P)^(2*N-1) -
#     2*N*P*(1+P)^(N+1) +
#     (1+P)^(N+1) -
#     2*N^2*P^2*(1+P)^(N) -
#     N*P*(1+P)^(N) -
#     2*N^2*P^2*(1+P)^(N-1) +
#     N*P*(1+P) +
#     N^2*P^2
# }
#
# denom <- (1 - (1+P)^(-N))
# 
# mean_estimator <- function(N, P) {
#   calc_mean <- (N*P) / denom)
#   return(calc_mean)
# }
# 
# 
# variance_estimator <- function(N, P) {
#   calc_var <- 
#     (N*P*(1+P) / denom) * 
#     (
#       1 - ((N*P)/1+P) * ((1 / (denom)) - 1)
#     )
#   return(calc_var)
# }
#
# mean_estimator(10, 0.5)
# 
# variance_estimator(10, 0.5)
```

# Optimization

```{r}
# ---------------------------------------------------------
# 1. The "Forward" Theoretical Functions (Eq 41.1 & 41.2)
# ---------------------------------------------------------

mean_tnb <- function(N, P) {
  Q <- 1 + P
  denom <- 1 - (Q^(-N))
  if (abs(denom) < 1e-15) return(Inf)
  
  # Eq 41.1
  return( (N * P) / denom )
}

variance_tnb <- function(N, P) {
  Q <- 1 + P
  denom <- 1 - (Q^(-N))
  if (abs(denom) < 1e-15) return(Inf)
  
  return(
    ((N * P * Q) / denom) *  (1 - ((N * P) / Q * ( (1/denom) - 1 )))
    )
}
```

mean estimated from r_ztbinom_book for N = 10, p = 0.5 = 4.84326 var
estimated from r_ztbinom_book for N = 10, p = 0.5 = 6.808941

```{r}
mean_tnb(10, 0.5)
variance_tnb(10, 0.5)
```

So not a perfect approximation, something to investigate or call it
close enough??

Initial values can either come from 

```{r}
# ---------------------------------------------------------
# 2. The Optimizer (Solving for N, P)
# ---------------------------------------------------------

find_parameters <- function(observed_mean, observed_var) {
  
  # Objective: Minimize the difference between Theoretical (Eq 41) and Observed
  objective_fn <- function(params) {
    
    N <- params[1]
    P <- params[2]
    
    theo_mean <- mean_tnb(N, P)
    theo_var  <- variance_tnb(N, P)
    
    # Squared Error Cost
    error <- ((theo_mean - observed_mean) / observed_mean)^2 + ((theo_var - observed_var) / observed_var)^2
    return(error)
  }
  
  # Initial Guess
  start_guesses <- c(observed_mean*0.9, (2.096 - 2.362*(observed_mean/observed_var)))
  
  result <- optim(
    par = start_guesses,
    fn = objective_fn,
    method = "L-BFGS-B",
    lower = c(0.001, 0.001), # Parameters must be positive
    upper = c(Inf, 0.9999)
  )
  
  return(list(
    Estimated_N = result$par[1],
    Estimated_P = result$par[2],
    Goodness_of_Fit = result$value # Closer to 0 is better
  ))
}
```

mean estimated from r_ztbinom_book for N = 10, p = 0.5 = 4.84326 var
estimated from r_ztbinom_book for N = 10, p = 0.5 = 6.808941

```{r}
find_parameters(4.84326, 6.808941)
```

This works as long as the variance isnt more than like 1.6 times the
mean and the variance isnt smaller than the mean.

```{r}
d <- r_ztbinom_book(n = 10000, p = 0.5, N = 10)

draws_test2 <- as.data.frame(d) |>
  rename(draws_test2 = d) 

draws_test2 |>
  ggplot(aes(x = draws_test2)) +
  geom_histogram(binwidth = 1) +
  theme_bw() 
```

```{r}
mean(draws_test2$draws_test2)
var(draws_test2$draws_test2)
```

```{r}
# ---------------------------------------------------------
# 3. Diagnostic Test Suite
# ---------------------------------------------------------

# Define a set of target Means and Variances to test
# Note: For Negative Binomial, Variance usually must be > Mean
test_scenarios <- tribble(
  ~Target_Mean, ~Target_Var,
  1.5,          5,
  4.8566,       6.693906,
  5.0957,          7.267268,
  10.5,         100,
  10.0,         11.0,
)

# Function to run a single diagnostic row
run_diagnostic <- function(t_mean, t_var) {
  
  # 1. Solve for N and P
  params <- find_parameters(t_mean, t_var)
  
  est_n <- params$Estimated_N
  est_p <- params$Estimated_P
  
  # 3. Generate Simulation
  # Using the user's r_ztbinom_book function
  # We generate a large sample size (n=50,000) to ensure stability of the sim mean/var
  sim_values <- r_ztbinom_book(n = 50000, p = est_p, N = est_n)
  
  # 4. Calculate Simulated Metrics
  sim_mean <- mean(sim_values)
  sim_var  <- var(sim_values)
  
  # 5. Return Row
  tibble(
    Target_Mean = t_mean,
    Target_Var  = t_var,
    Estimated_N = est_n,
    Estimated_P = est_p,
    check_Mean = sim_mean,
    check_Var = sim_var,
    # Calculate % error between Input Mean and Sim Mean
    Mean_Diff_Pct = paste0(round(((sim_mean - t_mean) / t_mean) * 100, 2), "%"),
    Var_Diff_Pct = paste0(round(((sim_var - t_var) / t_var) * 100, 2), "%")
  )
}

# Execute the tests across all scenarios
results_table <- test_scenarios |>
  pmap_dfr(function(Target_Mean, Target_Var) {
    run_diagnostic(Target_Mean, Target_Var)
  })

# ---------------------------------------------------------
# 4. Output Formatted Table
# ---------------------------------------------------------

results_table 
```
